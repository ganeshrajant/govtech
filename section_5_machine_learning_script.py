# -*- coding: utf-8 -*-
"""Section 5: Machine Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FfMWnhavGhm4fQLlTRtJXg1cWNNZ0AyY
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

sns.set_style("whitegrid")

# Read the dataset file
with open('car.data', 'r') as f:
  car_data = f.readlines()

# Preprocess and clean the data
cleaned_car_data = [data.replace('\n', '').split(',') for data in car_data]

# Create pandas dataframe from the cleaned car data
car_df = pd.DataFrame(cleaned_car_data, columns=["buying", "maint", "doors", "persons", "lug_boot", "safety", "class"])
car_df.head(5)

# Checking the details of the dataset
car_df.info()

# Checking for missing values
car_df.isnull().sum()

# EDA of the variables
# Count of the 'buying' target variable
sns.countplot(x=car_df['buying'])

# plotting unique counts of "maint"
sns.countplot(x=car_df["maint"])

# plotting unique counts of "doors"
sns.countplot(x=car_df["doors"])

# plotting unique counts of "persons"
sns.countplot(x=car_df["persons"])

# plotting unique counts of "lug_boot"
sns.countplot(x=car_df["lug_boot"])

# plotting unique counts of "safety"
sns.countplot(x=car_df["safety"])

# plotting unique counts of "class"
sns.countplot(x=car_df["class"])

# As our target variable is categorical we need to convert them to numerical

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

car_df["buying"] = le.fit_transform(car_df['buying'])
car_df.head()

# Split the variables and target variable
# X = car_df[car_df.columns[1:]]
X = car_df["class"].values.reshape(-1, 1)
y = car_df["buying"]

X = pd.get_dummies(X)
X.head()

# As all our variables looks ordinal categories, we will use ordinal number encoding 

maint_dict = {
    "vhigh": 3,
    "high": 2,
    "med": 1,
    "low": 0
}

doors_dict = {
    "2": 0,
    "3": 1,
    "4": 2,
    "5more": 3
}

persons_dict = {
    "2": 0,
    "4": 1,
    "more": 2
}

lug_boot_dict = {
    "small": 0,
    "med": 1,
    "big": 2
}

safety_dict = {
    "low": 0,
    "med": 1,
    "high": 2
}

class_dict = {
    "unacc": 0,
    "acc": 1,
    "good": 2,
    "vgood": 3
}

# Apply the above encoding dicts to the necessary columns
mapping_dict = [maint_dict, doors_dict, persons_dict, lug_boot_dict, safety_dict, class_dict]
for idx, col in enumerate(car_df.columns[1:]):
  print(mapping_dict[idx])
  car_df[col] = car_df[col].map(mapping_dict[idx])

car_df.head()

# Plot the correlation map of the variables
fig=plt.figure(figsize=(15,9))
sns.heatmap(car_df.corr(), annot=True)

"""If we look closely the correlation map then we can find out that the target value which is being asked for "buying" has no relation with other variables apart from the variable "class".

And according to the original dataset page [Car Evaluation](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation), the target variable is "class" not "buying" and if we check the correlation map of target "class" with other variables then we can find some correlation among them unlike withe the target variable "buying".

So here I am purposing 3 types of solution.

1. Take the target variable as "buying" as it is asked.
2. Try with the original target variable "class" which is asked in the original dataset page
3. As with "buying" only "class" has relation, try to model with only that variable.

## Target Variable: Buying
"""

# Split the variables and target variable
X = car_df[car_df.columns[1:]]
y = car_df["buying"]

# Split the X and y to train and test set

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

print(f"Shape of X_train: {X_train.shape} and y_train: {y_train.shape}")
print(f"Shape of X_test: {X_test.shape} and y_test: {y_test.shape}")

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import cross_val_score, learning_curve, validation_curve
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

from sklearn.model_selection import GridSearchCV

"""## KNN Classifier (Target: Buying)"""

knn = KNeighborsClassifier(n_jobs=-1)

knn.fit(X_train, y_train)

pred = knn.predict(X_test)
print(f"Accuracy of knn classifier is: {knn.score(X_test, y_test)}")

print(classification_report(y_test,pred))

"""So with KNN, we can only be able to achieve an accuracy of 0.16 whereas the F1 score stands at 0.17."""

# Trying KNN with different number of neightbours

avg_score=[]
for k in range(2,20):
    knn=KNeighborsClassifier(n_jobs=-1,n_neighbors=k)
    score=cross_val_score(knn,X_train,y_train,cv=5,n_jobs=-1,scoring='accuracy')
    avg_score.append(score.mean())

plt.figure(figsize=(10,5))
plt.plot(range(2,20, 1),avg_score)
plt.xlabel("n_neighbours")
plt.ylabel("accuracy")

"""**Conclusion:** From the above plot we can find out that even with neighbours=18, we can only reach 0.26 accuracy.

## Logistic Regression
"""

logclf=LogisticRegression()

logclf.fit(X_train,y_train)

pred=logclf.predict(X_test)

logclf.score(X_test,y_test)

"""With Basic default configs of Logistic Regression, we are able to get 0.31 accuracy."""

lc=learning_curve(logclf,X_train,y_train,cv=10,n_jobs=-1)
size=lc[0]
train_score=[lc[1][i].mean() for i in range (0,5)]
test_score=[lc[2][i].mean() for i in range (0,5)]
fig=plt.figure(figsize=(12,8))
plt.title("Learning curve using Logistic Regression")
plt.xlabel("Number of samples")
plt.ylabel("Accuracy")
plt.plot(size,train_score, label="Training")
plt.legend(loc='best')
plt.plot(size,test_score, label="Testing")
plt.legend(loc='best')

# Using Grid Search for hyperparameter Optimization
param_grid={'C':[0.01,0.1,1,10],
           'solver':['newton-cg', 'lbfgs', 'sag'],
           'multi_class':['multinomial']}
grid=GridSearchCV(estimator=LogisticRegression(n_jobs=-1),param_grid=param_grid,cv=5,n_jobs=-1)

grid.fit(X_train,y_train)

# Printing the best configs from the grid search
print(grid.best_params_)
print(grid.best_score_)

"""**Conclusion:** Even with the best configuration for the Logistic Regression our accuracy touches only 0.32

## Random Forest
"""

from sklearn.metrics import f1_score

rf_clf=RandomForestClassifier(n_jobs=-1,random_state=51)

rf_clf.fit(X_train,y_train)
print(f"Accuracy using Random Forest: {rf_clf.score(X_test,y_test)}")
print(f"F1 Score using Random Forest: {f1_score(y_test,rf_clf.predict(X_test),average='macro')}")

"""With Random Forest Classifier, we get only 0.07 accuracy which is really bad."""

# Using Grid Search for hyper parameter optimizatio
param_grid={'criterion':['gini','entropy'],
           'max_depth':[2,5,10,20],
           'max_features':[2,4,6,'auto'],
           'max_leaf_nodes':[2,3,None],}


grid=GridSearchCV(estimator=RandomForestClassifier(n_estimators=50,n_jobs=-1,random_state=51),
                  param_grid=param_grid,cv=10,n_jobs=-1)

grid.fit(X_train,y_train)

print(f"Random forest optimized configs for the parameters: {grid.best_params_}")
print(f"Best accuracy we got using best configs for Random forest: {grid.best_score_}")

lc=learning_curve(RandomForestClassifier(n_estimators=50,criterion='gini',max_features=4,max_depth=2,random_state=42,
                                             max_leaf_nodes=None,n_jobs=-1,),X_train,y_train,cv=5,n_jobs=-1)
size=lc[0]
train_score=[lc[1][i].mean() for i in range (0,5)]
test_score=[lc[2][i].mean() for i in range (0,5)]
fig=plt.figure(figsize=(12,8))
plt.title("Learning curve using Logistic Regression")
plt.xlabel("Number of samples")
plt.ylabel("Accuracy")
plt.plot(size,train_score, label="Training")
plt.legend(loc='best')
plt.plot(size,test_score, label="Testing")
plt.legend(loc='best')

feature_imp = pd.DataFrame(list(zip(X.columns, rf_clf.feature_importances_)), columns=["Feature", "Imp_score"])
feature_imp

# As safety is the least important feature we can drop that and train RF again
X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X[['maint', 'persons', 'lug_boot', 'safety', 'class']],
    y, test_size=0.25, random_state=42)

rf_clf_new=RandomForestClassifier(n_estimators=50,criterion='gini',max_features=4,max_depth=2,random_state=51,
    max_leaf_nodes=None,n_jobs=-1)
rf_clf_new.fit(X_train_new,y_train_new)
rf_clf_new.score(X_test_new,y_test_new)

"""**Conclusion** With feature importance and best Random forest configs we get the accuracy of 32.40

## Neural Network
"""

import tensorflow as tf

model = tf.keras.Sequential([
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(64, activation='relu'),
  tf.keras.layers.Dense(4)
])

model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

history = model.fit(X_train, y_train, validation_split=0.2, epochs=50)

def plot_loss(history):
  plt.plot(history.history['accuracy'], label='Accuracy')
  plt.plot(history.history['val_accuracy'], label='val_accuracy')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.legend()
  plt.grid(True)

plot_loss(history)

"""With Neural Network too the maximum accuracy on train set is 0.40 while on validation it was 0.27, and by looking at the graph it is clear that the dataset is overiftting.

**Final Conclusion** We tried simpler models to complex models and by far we have the best accuracy 0.32 without overfitting. So we will use that for our prediction.
"""

# Prediction
input = [{
    "maint": "high",
    "doors": "4",
    "persons": "2",
    "lug_boot": "big" ,
    "safety": "high",
    "class": "good"
}]
prediction_ex = pd.DataFrame(input)

for idx, col in enumerate(prediction_ex.columns):
  prediction_ex[col] = prediction_ex[col].map(mapping_dict[idx])

prediction_ex.head()

print(f"Buying price for the input values is: {le.inverse_transform(rf_clf.predict(prediction_ex))}")

"""## Using Target Variable "class" as mention in [Car Evaluation](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation)"""

# Split the variables and target variable
X = car_df[car_df.columns[:-1]]
y = car_df["class"]

# Split the X and y to train and test set

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

print(f"Shape of X_train: {X_train.shape} and y_train: {y_train.shape}")
print(f"Shape of X_test: {X_test.shape} and y_test: {y_test.shape}")

"""## KNN Classifier (Target: "Class")"""

knn = KNeighborsClassifier(n_jobs=-1)

knn.fit(X_train, y_train)

pred = knn.predict(X_test)
print(f"Accuracy of knn classifier is: {knn.score(X_test, y_test)}")

print(classification_report(y_test,pred))

"""So with KNN, we can able to achieve an accuracy of 0.87 whereas the F1 score stands at 0.70."""

# Trying KNN with different number of neightbours

avg_score=[]
for k in range(2,20):
    knn=KNeighborsClassifier(n_jobs=-1,n_neighbors=k)
    score=cross_val_score(knn,X_train,y_train,cv=5,n_jobs=-1,scoring='accuracy')
    avg_score.append(score.mean())

plt.figure(figsize=(10,5))
plt.plot(range(2,20, 1),avg_score)
plt.xlabel("n_neighbours")
plt.ylabel("accuracy")

"""**KNN Conclusion:** From the above plot we can find out that even with neighbours=5, we can only reach 0.90 accuracy.

## Logistic Regression
"""

logclf=LogisticRegression()

logclf.fit(X_train,y_train)

pred=logclf.predict(X_test)

print(f"Accuracy using Logistic Regression: {logclf.score(X_test,y_test)}")

"""With Basic default configs of Logistic Regression, we are able to get 0.73 accuracy."""

lc=learning_curve(logclf,X_train,y_train,cv=10,n_jobs=-1)
size=lc[0]
train_score=[lc[1][i].mean() for i in range (0,5)]
test_score=[lc[2][i].mean() for i in range (0,5)]
fig=plt.figure(figsize=(12,8))
plt.title("Learning curve using Logistic Regression")
plt.xlabel("Number of samples")
plt.ylabel("Accuracy")
plt.plot(size,train_score, label="Training")
plt.legend(loc='best')
plt.plot(size,test_score, label="Testing")
plt.legend(loc='best')

# Using Grid Search for hyperparameter Optimization
param_grid={'C':[0.01,0.1,1,10],
           'solver':['newton-cg', 'lbfgs', 'sag'],
           'multi_class':['multinomial']}
grid=GridSearchCV(estimator=LogisticRegression(n_jobs=-1),param_grid=param_grid,cv=5,n_jobs=-1)

grid.fit(X_train,y_train)

# Printing the best configs from the grid search
print(grid.best_params_)
print(grid.best_score_)

"""**LR Conclusion:** With the best configuration for the Logistic Regression our accuracy touches only 0.79

## Random Forest
"""

from sklearn.metrics import f1_score

rf_clf=RandomForestClassifier(n_jobs=-1,random_state=51)

rf_clf.fit(X_train,y_train)
print(f"Accuracy using Random Forest: {rf_clf.score(X_test,y_test)}")
print(f"F1 Score using Random Forest: {f1_score(y_test,rf_clf.predict(X_test),average='macro')}")

"""With Random Forest Classifier, we get only 0.96 accuracy and F1 0.89 which is very good compare to having target as "buying"."""

# Using Grid Search for hyper parameter optimizatio
param_grid={'criterion':['gini','entropy'],
           'max_depth':[2,5,10,20],
           'max_features':[2,4,6,'auto'],
           'max_leaf_nodes':[2,3,None],}


grid=GridSearchCV(estimator=RandomForestClassifier(n_estimators=50,n_jobs=-1,random_state=51),
                  param_grid=param_grid,cv=10,n_jobs=-1)

grid.fit(X_train,y_train)

print(f"Random forest optimized configs for the parameters: {grid.best_params_}")
print(f"Best accuracy we got using best configs for Random forest: {grid.best_score_}")

lc=learning_curve(RandomForestClassifier(n_estimators=50,criterion='entropy',max_features=4,max_depth=20,random_state=42,
                                             max_leaf_nodes=None,n_jobs=-1,),X_train,y_train,cv=5,n_jobs=-1)
size=lc[0]
train_score=[lc[1][i].mean() for i in range (0,5)]
test_score=[lc[2][i].mean() for i in range (0,5)]
fig=plt.figure(figsize=(12,8))
plt.title("Learning curve using Random Forest")
plt.xlabel("Number of samples")
plt.ylabel("Accuracy")
plt.plot(size,train_score, label="Training")
plt.legend(loc='best')
plt.plot(size,test_score, label="Testing")
plt.legend(loc='best')

feature_imp = pd.DataFrame(list(zip(X.columns, rf_clf.feature_importances_)), columns=["Feature", "Imp_score"])
feature_imp

# As doors is the least important feature we can drop that and train RF again
X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X[['buying', 'maint', 'persons', 'lug_boot', 'safety']],
    y, test_size=0.25, random_state=42)

rf_clf_new=RandomForestClassifier(n_estimators=50,criterion='entropy',max_features=4,max_depth=20,random_state=42,
    max_leaf_nodes=None,n_jobs=-1)
rf_clf_new.fit(X_train_new,y_train_new)
rf_clf_new.score(X_test_new,y_test_new)

"""**RF Conclusion** With feature importance and best Random forest configs we get the accuracy of 0.92, But earlier without feature importance we were able to achieve 0.97 with the best configs for Random Forest

## Neural Network
"""

model = tf.keras.Sequential([
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(64, activation='relu'),
  tf.keras.layers.Dense(4)
])

model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

history = model.fit(X_train, y_train, validation_split=0.2, epochs=50)

def plot_loss(history):
  plt.plot(history.history['accuracy'], label='loss')
  plt.plot(history.history['val_accuracy'], label='val_loss')
  plt.xlabel('Epoch')
  plt.ylabel('accuracy')
  plt.legend()
  plt.grid(True)

plot_loss(history)

"""With Neural Network too we werre able to achieve around 0.97-0.98 accuracy on train set while validation was around 0.95

**Final Conclusion** With all models having "class" as our target variable we were easily achieve the accuracy of 0.97.

## Target Variable: buying and Variable: Class

As we saw earliear in the heatmap, only "class" variable was showing any correlation with target "buying"  and with other variable it was zero, so we will try to build a model with only variable class.
"""

x = car_df["class"].values.reshape(-1, 1)
y = car_df["buying"]

# Split the X and y to train and test set

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

print(f"Shape of X_train: {X_train.shape} and y_train: {y_train.shape}")
print(f"Shape of X_test: {X_test.shape} and y_test: {y_test.shape}")

"""## KNN Classifier (Target: Buying)"""

knn = KNeighborsClassifier(n_jobs=-1)

knn.fit(X_train, y_train)

pred = knn.predict(X_test)
print(f"Accuracy of knn classifier is: {knn.score(X_test, y_test)}")

print(classification_report(y_test,pred))

"""So with KNN, we can only be able to achieve an accuracy of 0.23 whereas the F1 score stands at 0.16. Which is quite higher than taking all variables though in terms of accuracy."""

# Trying KNN with different number of neightbours

avg_score=[]
for k in range(2,20):
    knn=KNeighborsClassifier(n_jobs=-1,n_neighbors=k)
    score=cross_val_score(knn,X_train,y_train,cv=5,n_jobs=-1,scoring='accuracy')
    avg_score.append(score.mean())

plt.figure(figsize=(10,5))
plt.plot(range(2,20, 1),avg_score)
plt.xlabel("n_neighbours")
plt.ylabel("accuracy")

"""**Conclusion:** From the above plot we can find out that even with neighbours=4, we can only reach 0.29 accuracy.

## Logistic Regression
"""

logclf=LogisticRegression()

logclf.fit(X_train,y_train)

pred=logclf.predict(X_test)

logclf.score(X_test,y_test)

"""With Basic default configs of Logistic Regression, we are able to get 0.31 accuracy."""

lc=learning_curve(logclf,X_train,y_train,cv=10,n_jobs=-1)
size=lc[0]
train_score=[lc[1][i].mean() for i in range (0,5)]
test_score=[lc[2][i].mean() for i in range (0,5)]
fig=plt.figure(figsize=(12,8))
plt.title("Learning curve using Logistic Regression")
plt.xlabel("Number of samples")
plt.ylabel("Accuracy")
plt.plot(size,train_score, label="Training")
plt.legend(loc='best')
plt.plot(size,test_score, label="Testing")
plt.legend(loc='best')

# Using Grid Search for hyperparameter Optimization
param_grid={'C':[0.01,0.1,1,10],
           'solver':['newton-cg', 'lbfgs', 'sag'],
           'multi_class':['multinomial']}
grid=GridSearchCV(estimator=LogisticRegression(n_jobs=-1),param_grid=param_grid,cv=5,n_jobs=-1)

grid.fit(X_train,y_train)

# Printing the best configs from the grid search
print(grid.best_params_)
print(grid.best_score_)

"""**Conclusion:** Even with the best configuration for the Logistic Regression our accuracy is 0.30

## Random Forest
"""

from sklearn.metrics import f1_score

rf_clf=RandomForestClassifier(n_jobs=-1,random_state=51)

rf_clf.fit(X_train,y_train)
print(f"Accuracy using Random Forest: {rf_clf.score(X_test,y_test)}")
print(f"F1 Score using Random Forest: {f1_score(y_test,rf_clf.predict(X_test),average='macro')}")

"""With Random Forest Classifier, we get 0.34 accuracy which is an improvement over the random forest which was trained on every features."""

# Using Grid Search for hyper parameter optimizatio
param_grid={'criterion':['gini','entropy'],
           'max_depth':[2,5,10,20],
           'max_features':[2,4,6,'auto'],
           'max_leaf_nodes':[2,3,None],}


grid=GridSearchCV(estimator=RandomForestClassifier(n_estimators=50,n_jobs=-1,random_state=51),
                  param_grid=param_grid,cv=10,n_jobs=-1)

grid.fit(X_train,y_train)

print(f"Random forest optimized configs for the parameters: {grid.best_params_}")
print(f"Best accuracy we got using best configs for Random forest: {grid.best_score_}")

lc=learning_curve(RandomForestClassifier(n_estimators=50,criterion='gini',max_features='auto',max_depth=2,random_state=42,
                                             max_leaf_nodes=None,n_jobs=-1,),X_train,y_train,cv=5,n_jobs=-1)
size=lc[0]
train_score=[lc[1][i].mean() for i in range (0,5)]
test_score=[lc[2][i].mean() for i in range (0,5)]
fig=plt.figure(figsize=(12,8))
plt.title("Learning curve using Random Forest")
plt.xlabel("Number of samples")
plt.ylabel("Accuracy")
plt.plot(size,train_score, label="Training")
plt.legend(loc='best')
plt.plot(size,test_score, label="Testing")
plt.legend(loc='best')

"""**Conclusion** With having random forest best configs we were able to achieve 0.34 accuracy.

## Neural Network
"""

model = tf.keras.Sequential([
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(64, activation='relu'),
  tf.keras.layers.Dense(4)
])

model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

history = model.fit(X_train, y_train, validation_split=0.2, epochs=50)

def plot_loss(history):
  plt.plot(history.history['accuracy'], label='Accuracy')
  plt.plot(history.history['val_accuracy'], label='val_accuracy')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.legend()
  plt.grid(True)

plot_loss(history)

"""**NN Conclusing:** So after training Neural network we got around 0.32 accuracy on training and 0.31 on validation.

**Conclusion**: Having just one feature we manage to get 0.34 at best which is quite similar to what we have already training with all features. Which shows that other variables with having target variable "buying" doesn't contribute anything to the training whereas taking the target "class" we were able to get an accuracy of 0.97 which is quite larger than what we have with it's counterpart.
"""